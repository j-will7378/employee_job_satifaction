# -*- coding: utf-8 -*-
"""Employee_CSAT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xYx10Hy3X5XpCRvXFbNBBVHyEnxq0HL6
"""

!pip install --upgrade numpy
!pip install --upgrade scikit-learn-extra

!pip install scikit-learn-extra

!pip install --upgrade --no-build-isolation scikit-learn-extra

!pip install --upgrade --force-reinstall numpy
!pip install --upgrade --force-reinstall scikit-learn
!pip install --upgrade --force-reinstall scikit-learn-extra

!pip install --upgrade --force-reinstall numpy scikit-learn

# Libraries to help with reading and manipulating data
import numpy as np
import pandas as pd

# Libraries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_theme(style='darkgrid')

# Removes the limit for the number of displayed columns
pd.set_option("display.max_columns", None)
# Sets the limit for the number of displayed rows
pd.set_option("display.max_rows", 200)

# To scale the data using z-score
from sklearn.preprocessing import StandardScaler

# To compute distances
from scipy.spatial.distance import cdist, pdist

# To perform K-Means clustering and compute silhouette scores
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# To import K-Medoids
#from sklearn_extra.cluster import KMedoids

# To import DBSCAN and Gaussian Mixture
from sklearn.cluster import DBSCAN
from sklearn.mixture import GaussianMixture

# To perform hierarchical clustering, compute cophenetic correlation, and create dendrograms
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

df= pd.read_csv('/content/drive/MyDrive/Employee_Survey/employee_survey.csv')

df.head()

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df.head(1)

df[['Gender', 'MaritalStatus', 'Dept', 'EmpType']].value_counts()

df['Gender'].value_counts()

df.describe(include="all")

# prompt: I want to create age groups

# Create age groups
bins = [18, 25, 35, 45, 55, 65]  # Define age group boundaries
labels = ['18-24', '25-34', '35-44', '45-54', '55-64']  # Assign labels to each group
df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)

df['AgeGroup'].value_counts()

"""# Observations
- There are more men.
- 25 - 34 is the top age group
- Most employees are full time.
- Most work in IT
- The overall top group are married men, who are full time and work in IT.
- The top female group are fulltime and work in finanace.
- IT and finance have the most employees
"""

df['Dept'].value_counts()

# Function to plot a boxplot and a histogram along the same scale


def histogram_boxplot(data, feature, figsize = (12, 7), kde = False, bins = None):

    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12, 7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """

    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows = 2,      # Number of rows of the subplot grid = 2
        sharex = True,  # X-axis will be shared among all subplots
        gridspec_kw = {"height_ratios": (0.25, 0.75)},
        figsize = figsize,
    )  # Creating the 2 subplots
    sns.boxplot(
        data = data, x = feature, ax = ax_box2, showmeans = True, color = "violet"
    )  # Boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data = data, x = feature, kde = kde, ax = ax_hist2, bins = bins, palette = "winter"
    ) if bins else sns.histplot(
        data = data, x = feature, kde = kde, ax = ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color = "green", linestyle = "--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color = "black", linestyle = "-"
    )  # Add median to the histogram

df.head(1)

histogram_boxplot(df, 'PhysicalActivityHours')

histogram_boxplot(df, 'SleepHours')

histogram_boxplot(df, 'CommuteDistance')

histogram_boxplot(df, 'NumCompanies')

histogram_boxplot(df, 'TrainingHoursPerYear')

histogram_boxplot(df, 'TeamSize')

histogram_boxplot(df, 'Experience')

"""# Onservation
- Employees get around 2 hrs of physical activity per day.
- Employees average 7 hrs of sleep per night.
- Average commute distance is around 13 miles.
- Average team size is 16.5 employees per team.
- Employees average 37 hrs of training per year.
- Employees on average have 9 yrs of experience.

### How do these effect overall Job satis faction?
"""

# Function to create labeled barplots


def labeled_barplot(data, feature, perc = False, n = None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # Length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize = (count + 1, 5))
    else:
        plt.figure(figsize = (n + 1, 5))

    plt.xticks(rotation = 45, fontsize = 15)
    ax = sns.countplot(
        data = data,
        x = feature,
        palette = "Paired",
        order = data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )                       # Percentage of each class of the category
        else:
            label = p.get_height()  # Count of each level of the category

        x = p.get_x() + p.get_width() / 2  # Width of the plot
        y = p.get_height()                 # Height of the plot

        ax.annotate(
            label,
            (x, y),
            ha = "center",
            va = "center",
            size = 12,
            xytext = (0, 5),
            textcoords = "offset points",
        )  # Annotate the percentage

    plt.show()  # Show the plot

df.head(1)

labeled_barplot(df, 'JobSatisfaction')

labeled_barplot(df, 'EduLevel')

labeled_barplot(df, 'JobLevel')

labeled_barplot(df, 'WLB')

labeled_barplot(df, 'WorkEnv')

labeled_barplot(df, 'Workload')

labeled_barplot(df, 'Stress')

"""# Observations
- Most rate overall job satisfactiopn at 4
- Work life balance, workload, and work enviroment ratings are uniformly distrubuted.
- Work stress is rated high by most.

# Bivariate Analysis
"""

# Correlation check
cols_list = df.select_dtypes(include = np.number).columns.tolist()

plt.figure(figsize = (15, 7))

sns.heatmap(
    df[cols_list].corr(), annot = True, vmin = -1, vmax = 1, fmt = ".2f", cmap = "Spectral"
)

plt.show()

df.head(1)

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'AgeGroup', y = 'JobSatisfaction', ci = False)
plt.xticks(rotation = 90)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'Dept', y = 'JobSatisfaction')
plt.xticks(rotation = 90)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'JobLevel', y = 'JobSatisfaction', ci = False)
plt.xticks(rotation = 90)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'EmpType', y = 'JobSatisfaction', ci = False)
plt.xticks(rotation = 90)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'CommuteMode', y = 'JobSatisfaction', ci = False)
plt.xticks(rotation = 90)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'CommuteDistance', y = 'JobSatisfaction', ci = False)
plt.xticks(rotation = 90)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'PhysicalActivityHours', y = 'JobSatisfaction', ci = False)
plt.xticks(rotation = 90)
plt.show()

plt.figure(figsize = (15,5))
sns.barplot(data = df, x = 'SleepHours', y = 'JobSatisfaction', ci = False)
plt.xticks(rotation = 90)
plt.show()

# Let's plot the boxplots of all numerical columns to check for outliers.
plt.figure(figsize = (20, 10))

numeric_columns = df.select_dtypes(include = np.number).columns.tolist()

for i, variable in enumerate(numeric_columns):

    plt.subplot(2, 5, i + 1)

    plt.boxplot(df[variable], whis = 1.5)

    plt.tight_layout()

    plt.title(variable)

plt.show()

# Convert Likert scale variables to categorical
likert_cols = ['JobSatisfaction', 'EduLevel', 'JobLevel', 'WLB', 'WorkEnv', 'Workload', 'Stress']
for col in likert_cols:
    df[col] = pd.Categorical(df[col])

df.info()

"""# Bayesian Analysis"""

!pip install bambi

# Install libraries if you don't have them
# !pip install pymc arviz bambi pandas numpy matplotlib seaborn

import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import bambi as bmb
import matplotlib.pyplot as plt
import seaborn as sns

print(f"Running on PyMC v{pm.__version__}")
print(f"Running on Bambi v{bmb.__version__}")
print(f"Running on ArviZ v{az.__version__}")

# --- Load your data here ---
# Assuming your DataFrame is named 'df'
# Example: df = pd.read_csv('your_employee_data.csv')

# --- Data Preparation ---

# Check the actual categories and their order for JobSatisfaction
print("JobSatisfaction Categories:", df['JobSatisfaction'].cat.categories)
print("JobSatisfaction dtype:", df['JobSatisfaction'].dtype)

# IMPORTANT: For Bayesian Ordinal Regression, the target variable needs to be
# numeric, typically integers starting from 0 representing the ordered levels.
# If your categories are like ['Low', 'Medium', 'High'], map them to [0, 1, 2].
# If they are already [1, 2, 3, 4, 5], you might need to subtract 1 if the
# model expects 0-based indexing, or adjust the model call. Let's assume
# they are ordered categories that need conversion to numeric codes (0-based).

# Example conversion (ADAPT THIS based on your actual categories):
satisfaction_mapping = {cat: i for i, cat in enumerate(df['JobSatisfaction'].cat.categories)}
# Or if they are already numeric 1-5: satisfaction_mapping = {i: i-1 for i in range(1, 6)}

df['JobSatisfaction_Ordinal'] = df['JobSatisfaction'].map(satisfaction_mapping).astype(int)
print("\nCheck numeric mapping:")
print(df[['JobSatisfaction', 'JobSatisfaction_Ordinal']].head())
print("\nUnique values in JobSatisfaction_Ordinal:", df['JobSatisfaction_Ordinal'].unique())


# Convert object columns that are categorical to 'category' type if not already done
# This helps some modeling libraries recognize them automatically.
for col in ['Gender', 'MaritalStatus', 'Dept', 'EmpType', 'CommuteMode']:
     if df[col].dtype == 'object':
        df[col] = df[col].astype('category')

# Optional: Scale numerical predictors (can help MCMC sampling performance)
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
# Remove target and identifiers if they are numeric
numerical_cols = [c for c in numerical_cols if c not in ['EmpID', 'JobSatisfaction_Ordinal', 'NumReports', 'NumCompanies', 'TeamSize']] # Example exclusions
# Scale relevant ones
# for col in numerical_cols:
#     df[col] = (df[col] - df[col].mean()) / df[col].std()
# print("\nNumerical columns scaled (example):", numerical_cols[:2])


# Check dtypes again after potential conversions
print("\nData types after preparation:")
print(df.info())

# Define number of satisfaction levels
num_satisfaction_levels = df['JobSatisfaction_Ordinal'].nunique()
print(f"\nNumber of Job Satisfaction levels: {num_satisfaction_levels}")

# Example: Relationship between WLB and JobSatisfaction
sns.countplot(data=df, x='WLB', hue='JobSatisfaction')
plt.title('Job Satisfaction distribution by Work-Life Balance')
plt.show()

# Example: Relationship between Experience and JobSatisfaction (numeric vs categorical)
sns.boxplot(data=df, x='JobSatisfaction', y='Experience')
plt.title('Experience distribution by Job Satisfaction')
plt.show()

df.head(1)

# Example: Relationship between WLB and JobSatisfaction
sns.countplot(data=df, x='WorkEnv', hue='JobSatisfaction')
plt.title('Job Satisfaction distribution by Work Enviroment')
plt.show()

# Example: Relationship between Experience and JobSatisfaction (numeric vs categorical)
sns.boxplot(data=df, x='JobSatisfaction', y='SleepHours')
plt.title('Sleep hours distribution by Job Satisfaction')
plt.show()

# Example: Relationship between WLB and JobSatisfaction
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Dept', hue='JobSatisfaction')
plt.title('Job Satisfaction distribution by Department')
plt.xticks(rotation=45)
plt.show()

# Example: Relationship between Experience and JobSatisfaction (numeric vs categorical)
sns.boxplot(data=df, x='JobSatisfaction', y='CommuteDistance')
plt.title('Commute distribution by Job Satisfaction')
plt.show()

# Example: Relationship between WLB and JobSatisfaction
sns.countplot(data=df, x='Stress', hue='JobSatisfaction')
plt.title('Job Satisfaction distribution by Stress')
plt.show()

# Example: Relationship between Experience and JobSatisfaction (numeric vs categorical)
sns.boxplot(data=df, x='JobSatisfaction', y='TeamSize')
plt.title('Team Size distribution by Job Satisfaction')
plt.show()

# Example: Relationship between WLB and JobSatisfaction
sns.countplot(data=df, x='EduLevel', hue='JobSatisfaction')
plt.title('Job Satisfaction distribution by Education Level')
plt.show()

# Example: Relationship between Experience and JobSatisfaction (numeric vs categorical)
sns.boxplot(data=df, x='JobSatisfaction', y='PhysicalActivityHours')
plt.title('Physical Activity Hours by Job Satisfaction')
plt.show()

# Example: Relationship between WLB and JobSatisfaction
sns.countplot(data=df, x='AgeGroup', hue='JobSatisfaction')
plt.title('Job Satisfaction distribution by Age Group')
plt.show()

# --- Define and Fit the Model ---

# Select a subset of predictors for demonstration. You can add more.
# Choose predictors based on domain knowledge or EDA.
# Bambi automatically handles dummy coding for categorical predictors.
model_formula = "JobSatisfaction_Ordinal ~ WLB + WorkEnv + Workload + Stress + Experience + haveOT + Gender + Age"

# Create the Bambi model
# Priors: Bambi sets default weakly informative priors, which is often a good start.
# Link function: 'logit' specifies the ordinal *logistic* regression type.
# Family: Use 'ordinal' for ordered categorical response variables.
try:
    # CORRECTED LINE: Use family="ordinal"
    model = bmb.Model(model_formula, df, family="ordinal", link="logit") # <-- CHANGE HERE

    # Fit the model using MCMC sampling
    # draws: Number of samples per chain after warmup
    # tune: Number of warmup/tuning steps (discarded)
    # chains: Number of independent chains to run
    # target_accept: Controls sampler step size adaptation (0.8 is default, increase if divergences occur)
    # idata_kwargs: arguments passed to Arviz when creating the InferenceData object
    idata = model.fit(draws=1000, tune=1000, chains=4, random_seed=42, idata_kwargs={'log_likelihood': True})

    # Display model structure
    print(model)

except Exception as e:
    print(f"An error occurred during model building or fitting: {e}")
    print("Check variable names, data types, and potential issues like perfect separation.")
    idata = None # Ensure idata is None if fitting fails

import bambi as bmb
print(f"Using Bambi version: {bmb.__version__}")

import pandas as pd
import numpy as np
import pymc as pm
import arviz as az
import bambi as bmb
import matplotlib.pyplot as plt
import seaborn as sns

# Import the specific family class
from bambi.families import Ordinal # <-- ADD THIS IMPORT

print(f"Running on PyMC v{pm.__version__}")
print(f"Running on Bambi v{bmb.__version__}")
print(f"Running on ArviZ v{az.__version__}")

# --- Load and prepare your data 'df' here ---
# Ensure df['JobSatisfaction_Ordinal'] exists and is 0-indexed numeric

# --- Define and Fit the Model ---
model_formula = "JobSatisfaction_Ordinal ~ WLB + WorkEnv + Workload + Stress + Experience + haveOT + Gender + Age"

try:
    # CORRECTED LINE: Instantiate the Ordinal class directly
    # Pass the link function ('logit') when creating the Ordinal object
    model = bmb.Model(model_formula, df, family=Ordinal(link="logit")) # <-- CHANGE HERE

    # Fit the model using MCMC sampling
    idata = model.fit(draws=1000, tune=1000, chains=4, random_seed=42, idata_kwargs={'log_likelihood': True})

    # Display model structure
    print(model)

except Exception as e:
    print(f"An error occurred during model building or fitting: {e}")
    print("Check variable names, data types, and potential issues like perfect separation.")
    idata = None # Ensure idata is None if fitting fails

# --- Rest of your analysis code (diagnostics, interpretation) ---
# ... (Code from Step 4 and Step 5 previously provided) ...

import bambi as bmb

print("--- Contents of bambi.families ---")
try:
    # List names directly available in the families module
    print(dir(bmb.families))
except Exception as e:
    print(f"Could not inspect bambi.families: {e}")

print("\n--- Contents of bambi top-level ---")
try:
    # Also check the top-level bambi module
    print(dir(bmb))
except Exception as e:
    print(f"Could not inspect bambi: {e}")

# --- Minimal Test ---
try:
    print("\nRunning minimal Bambi ordinal model test...")
    # Ensure 'JobSatisfaction_Ordinal' exists in df and is numeric 0-indexed
    minimal_model = bmb.Model("JobSatisfaction_Ordinal ~ 1", df, family="ordinal", link="logit")
    print("Minimal model built successfully!")
    # You don't need to fit it, just see if building works
    # idata_minimal = minimal_model.fit(draws=100, tune=100) # Optional: short fit
    # print("Minimal model fitted successfully!")
except Exception as e:
    print(f"Minimal model failed: {e}")
# --- End Minimal Test ---

# Ensure necessary libraries are imported
import pymc as pm
import pandas as pd
import numpy as np
import arviz as az
import patsy # Used for formula parsing
import matplotlib.pyplot as plt # For plots later

# --- Ensure 'df' is your prepared DataFrame ---
# Ensure 'JobSatisfaction_Ordinal' is your 0-indexed target variable

# --- Define your model formula ---
model_formula = "JobSatisfaction_Ordinal ~ WLB + WorkEnv + Workload + Stress + Experience + haveOT + Gender + Age"

# Use patsy to create design matrices from the formula
# This automatically handles dummy coding for categorical variables
print("Creating design matrices using Patsy...")
try:
    y, X = patsy.dmatrices(model_formula, data=df, return_type='dataframe')
    print("Design matrices created successfully.")
except Exception as e:
    print(f"Error creating design matrices with Patsy: {e}")
    print("Check your formula and DataFrame column names/types.")
    # Stop execution if patsy fails
    raise e # Or handle more gracefully

# Get predictor names from the design matrix (includes intercept)
predictor_names = X.columns.tolist()
print("Predictors in design matrix:", predictor_names)

# Get the number of categories and predictors
num_categories = df['JobSatisfaction_Ordinal'].nunique()
num_predictors = X.shape[1] # Number of columns in X (including intercept)

print(f"Number of satisfaction categories: {num_categories}")
print(f"Number of predictors (including intercept): {num_predictors}")

from sklearn.preprocessing import StandardScaler

numerical_cols = ['Age', 'Experience', 'PhysicalActivityHours', 'SleepHours', 'CommuteDistance', 'TeamSize', 'TrainingHoursPerYear'] # Adjust list as needed
numerical_cols = [col for col in numerical_cols if col in df.columns] # Ensure columns exist

if numerical_cols:
    scaler = StandardScaler()
    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])
    print("Numerical columns standardized.")
else:
    print("No numerical columns specified or found for scaling.")

# Now create design matrices
# y, X = patsy.dmatrices(model_formula, data=df, return_type='dataframe')
# ... etc ...


# --- Define the PyMC Model ---
with pm.Model() as pymc_ordinal_model:
    print("\nBuilding PyMC model with tighter priors...")
    # --- Priors (TIGHTER) ---
    betas = pm.Normal('betas', mu=0, sigma=1.5, shape=num_predictors) # Smaller sigma
    cutpoints = pm.Normal('cutpoints', mu=0, sigma=1.5, shape=num_categories - 1, # Smaller sigma
                          transform=pm.distributions.transforms.ordered)
    # ... rest of model definition ...# Adjusted sigma


    # --- Linear Predictor ---
    # Calculate the latent variable 'eta' (often called mu previously)
    # Corrected indentation: 4 spaces
    eta = pm.math.dot(X.values.astype(np.float64), betas) # Ensure X is float for dot product

    # --- Likelihood ---
    # Use the OrderedLogistic distribution from PyMC
    # Observed data is 'y.values.flatten()' which contains the 0, 1, 2... categories
    y_obs = pm.OrderedLogistic('y_obs', eta=eta, cutpoints=cutpoints,
                               observed=y.values.flatten().astype(int)) # Ensure y is integer

    print("PyMC model built.")

# --- Sampling (Execute the MCMC) ---
# This is usually done outside the 'with pm.Model()' block
# --- Sampling (Execute the MCMC) ---
print("\nStarting PyMC sampling with initvals...")
try:
# Define reasonable starting points for the 4 cutpoints
  init_cutpoints = np.array([-1.5, -0.5, 0.5, 1.5]) # Explicit start
  idata_pymc = pm.sample(
        draws=1000,
        tune=1000,
        chains=4,
        model=pymc_ordinal_model,
        random_seed=42,
        target_accept=0.9,
        initvals={'cutpoints': init_cutpoints}
        # --- ADD THIS ARGUMENT ---

    )

    # Add posterior predictive samples
  idata_pymc.extend(pm.sample_posterior_predictive(idata_pymc, model=pymc_ordinal_model))
  print("Sampling completed.")

# --- Keep the rest of the except block and analysis code ---
except Exception as e:
    print(f"An error occurred during PyMC sampling. Specific error below:")
    print(e)
    idata_pymc = None

# --- Analysis (using ArviZ, same as before) ---
if idata_pymc:
    print("\n--- PyMC Model Summary ---")
    # Exclude posterior predictive samples from summary by default
    summary_vars = ['betas', 'cutpoints']
    summary_pymc = az.summary(idata_pymc, var_names=summary_vars, hdi_prob=0.94)

    # Add predictor names back to the summary for easier interpretation
    # Note: The order of 'betas' corresponds to the order in X.columns
    beta_indices = [idx for idx in summary_pymc.index if idx.startswith('betas')]
    cutpoint_indices = [idx for idx in summary_pymc.index if idx.startswith('cutpoints')]

    named_indices = []
    # Name the betas
    for i, idx in enumerate(beta_indices):
         # Assuming beta_indices are like 'betas[0]', 'betas[1]', etc.
         predictor_index = int(idx.split('[')[1][:-1])
         named_indices.append(('beta', predictor_names[predictor_index]))
    # Name the cutpoints (less critical but for completeness)
    for i, idx in enumerate(cutpoint_indices):
         named_indices.append(('cutpoint', f'cutpoint_{i}')) # Simple naming

    if len(named_indices) == len(summary_pymc):
         summary_pymc.index = pd.MultiIndex.from_tuples(named_indices, names=['parameter_type', 'predictor/cutpoint'])
         print("\n--- PyMC Model Summary with Predictor Names ---")
         print(summary_pymc)
    else:
         print("\n--- PyMC Model Summary (without predictor names mapping) ---")
         print(summary_pymc)


    # --- Plotting (Trace, Posterior) ---
    print("\nGenerating diagnostic plots...")
    az.plot_trace(idata_pymc, var_names=summary_vars, compact=True)
    plt.tight_layout()
    plt.show()

    az.plot_posterior(idata_pymc, var_names=['betas'], hdi_prob=0.94) # Plot only betas
    plt.tight_layout()
    plt.show()

    # Interpretation follows the same logic as before, looking at the 'betas' HDIs
    # (You can copy the interpretation print loop from the earlier Bambi steps,
    # just make sure you are referencing the correct summary DataFrame 'summary_pymc'
    # and accessing the HDI columns like 'hdi_3%' and 'hdi_97%')

else:
    print("\nSkipping analysis as PyMC sampling failed or was not performed.")

"""# K-Means Clustering"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Assuming 'df' is your original DataFrame with correct dtypes set previously

# --- Step 1: Select Features for Clustering ---
# Identify your numerical and categorical PREDICTORS
# Exclude EmpID, JobSatisfaction*, AgeGroup

numerical_predictors = [
    'Age', 'Experience', 'PhysicalActivityHours', 'SleepHours',
    'CommuteDistance', 'NumCompanies', 'TeamSize', 'NumReports',
    'TrainingHoursPerYear'
]

# Include all category/object types intended as predictors
# Ensure JobLevel, WLB, WorkEnv, Workload, Stress, EduLevel are 'category' type if using codes,
# BUT using one-hot encoding is generally safer for K-means/PCA unless order is critical.
# Let's plan to one-hot encode all non-boolean categorical predictors.
categorical_predictors = [
    'Gender', 'MaritalStatus', 'JobLevel', 'Dept', 'EmpType',
    'WLB', 'WorkEnv', 'Workload', 'Stress', 'CommuteMode', 'EduLevel'
]

boolean_predictor = 'haveOT'

# Select only the predictor columns from the original DataFrame
df_predictors = df[numerical_predictors + categorical_predictors + [boolean_predictor]].copy()


# --- Step 2: Encode Categorical Features (One-Hot Encoding) ---
# Use pd.get_dummies for one-hot encoding
# drop_first=False is usually fine for PCA/K-means
df_encoded = pd.get_dummies(df_predictors, columns=categorical_predictors, drop_first=False)


# --- Step 3: Handle Boolean ---
# Convert boolean column to int (it might already be included in df_encoded if
# pd.get_dummies processed it, check dtypes if unsure)
# If 'haveOT' was boolean and NOT processed by get_dummies:
if boolean_predictor in df_encoded.columns and df_encoded[boolean_predictor].dtype == 'bool':
    df_encoded[boolean_predictor] = df_encoded[boolean_predictor].astype(int)
    print(f"Column '{boolean_predictor}' converted to integer.")
# Note: pd.get_dummies might automatically convert boolean to 0/1 if dtype='category' wasn't set

# Ensure all columns are now numeric
print("\nChecking dtypes after encoding:")
print(df_encoded.info()) # Should show only numeric types (int, float, uint8)


# --- Step 4: Scale the Numerical Data ---
# Now that ALL columns in df_encoded are numeric, we can scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded)

# Convert back to DataFrame for clarity (optional, PCA/KMeans can take the numpy array)
X_scaled_df = pd.DataFrame(X_scaled, index=df_encoded.index, columns=df_encoded.columns)

print("\nData successfully encoded and scaled. Shape:", X_scaled_df.shape)
print("Sample of scaled data:")
print(X_scaled_df.head())


# --- Now you can proceed with PCA and K-means ---

# Example: PCA
# pca = PCA(n_components=0.95) # Keep 95% of variance, or choose fixed number
# X_pca = pca.fit_transform(X_scaled_df)
# print(f"\nPCA completed. Data shape after PCA: {X_pca.shape}")

# Example: K-means (on scaled data OR PCA results)
# # Determine optimal K using Elbow method or Silhouette score (run this first)
# # ... code to find optimal k ...
# optimal_k = 5 # Replace with your chosen K
# kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
# cluster_labels = kmeans.fit_predict(X_scaled_df) # Or fit_predict(X_pca)

# Add cluster labels back to original df (or a copy) for analysis
# df['Cluster'] = cluster_labels

# Analyze clusters - e.g., calculate mean JobSatisfaction per cluster
# satisfaction_by_cluster = df.groupby('Cluster')['JobSatisfaction_Ordinal'].mean() # Or use original categories
# print("\nAverage Satisfaction by Cluster:")
# print(satisfaction_by_cluster)

"""### Scaling"""

# Clustering algorithms are distance-based algorithms, and all distance-based algorithms are affected by the scale of the variables. Therefore, we will scale the data before applying clustering.

# Scaling the data before clustering
scaler = StandardScaler()
subset = X_scaled_df.iloc[:, 3:].copy()
subset_scaled = scaler.fit_transform(subset)

# Creating a dataframe of the scaled data
subset_scaled_df = pd.DataFrame(subset_scaled, columns = subset.columns)

"""### Applying PCA"""

#PCA can help to mitigate the effects of collinearity by identifying the most important variables or features that explain the maximum variance in the data. The principal components generated by PCA are uncorrelated with each other, which can reduce the redundancy in the data and can make the clustering more robust.

# Importing PCA
from sklearn.decomposition import PCA

# Defining the number of principal components to generate
n = subset.shape[1]                                                 # Storing the number of variables in the data

pca = PCA(n_components = n, random_state = 1)                       # Storing PCA function with n components

data_pca = pd.DataFrame(pca.fit_transform(subset_scaled_df ))       # Applying PCA on scaled data

# The percentage of variance explained by each principal component is stored
exp_var = (pca.explained_variance_ratio_)

"""# K-Means Clustering

- K-Means clustering is one of the most popular clustering algorithms used for partitioning a dataset into K clusters. The algorithm works by iteratively assigning each data point to one of the K clusters based on the proximity of the data points to the centroids of the clusters. K-Means clustering is a computationally efficient algorithm that can work well even for datasets with a large number of variables.

- The steps involved in K-Means clustering are as follows:

  - Choose the number of clusters K that you want to partition the data into.
  - Initialize the K centroids randomly.
  - Assign each data point to the nearest centroid.
  - Recalculate the centroids of each cluster as the mean of all the data points assigned to it.
  - Repeat steps 3 and 4 until the centroids no longer change or a maximum number of iterations is reached.

"""

k_means_df = data_pca.copy()

clusters = range(1, 15)
meanDistortions = []

for k in clusters:
    model = KMeans(n_clusters = k, random_state = 1)
    model.fit(data_pca)
    prediction = model.predict(k_means_df)
    distortion = (
        sum(np.min(cdist(k_means_df, model.cluster_centers_, "euclidean"), axis = 1))
        / k_means_df.shape[0]
    )

    meanDistortions.append(distortion)

    print("Number of Clusters:", k, "\tAverage Distortion:", distortion)

plt.plot(clusters, meanDistortions, "bx-")
plt.xlabel("k")
plt.ylabel("Average Distortion")
plt.title("Selecting k with the Elbow Method", fontsize = 20)
plt.show()

"""### Looks like after K = 4 the curve starts to level off."""

kmeans = KMeans(n_clusters = 4, random_state = 1)
kmeans.fit(k_means_df)

# Creating a copy of the original data
df1 = df.copy()

# Adding K-Means cluster labels to the K-Means and original dataframes
k_means_df["KM_segments"] = kmeans.labels_
df1["KM_segments"] = kmeans.labels_

"""### Cluster Profiles"""

km_cluster_profile = df1.groupby("KM_segments").mean()

df1.head(1)

# Creating the "count_in_each_segment" feature in K-Means cluster profile

km_cluster_profile["count_in_each_segment"] = (
    df1.groupby("KM_segments")["JobSatisfaction_Ordinal"].count().values
)

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
# Make sure scikit-learn is installed: !pip install scikit-learn

# Assume 'df' is your original DataFrame loaded with the survey data
# and includes 'JobSatisfaction_Ordinal' and other relevant columns.

# --- 1. Data Preparation for Clustering ---

# --- 1a. Select Predictor Features ---
# Choose columns to base the clusters on. Exclude IDs and the target variable.
numerical_predictors = [
    'Age', 'Experience', 'PhysicalActivityHours', 'SleepHours',
    'CommuteDistance', 'NumCompanies', 'TeamSize', 'NumReports',
    'TrainingHoursPerYear'
]
categorical_predictors = [
    'Gender', 'MaritalStatus', 'JobLevel', 'Dept', 'EmpType',
    'WLB', 'WorkEnv', 'Workload', 'Stress', 'CommuteMode', 'EduLevel'
]
boolean_predictor = 'haveOT'

# Ensure specified predictor columns exist in the DataFrame
all_predictors = numerical_predictors + categorical_predictors + [boolean_predictor]
missing_cols = [col for col in all_predictors if col not in df.columns]
if missing_cols:
    print(f"Warning: The following predictor columns are missing from df: {missing_cols}")
    # Adjust predictor lists if necessary or raise an error
    # For now, filter lists to only include existing columns
    numerical_predictors = [col for col in numerical_predictors if col in df.columns]
    categorical_predictors = [col for col in categorical_predictors if col in df.columns]
    if boolean_predictor not in df.columns:
         boolean_predictor = None # Set to None if missing
         print(f"Warning: Boolean predictor '{boolean_predictor}' not found.")

# Select only the relevant predictor columns
df_predictors = df[numerical_predictors + categorical_predictors + ([boolean_predictor] if boolean_predictor else [])].copy()
print(f"Selected {df_predictors.shape[1]} predictor columns.")

# --- 1b. Encode Categorical Features (One-Hot Encoding) ---
if categorical_predictors:
    print("Applying One-Hot Encoding...")
    df_encoded = pd.get_dummies(df_predictors, columns=categorical_predictors, drop_first=False)
else:
    df_encoded = df_predictors # No categorical columns to encode

# --- 1c. Handle Boolean Feature ---
if boolean_predictor and boolean_predictor in df_encoded.columns:
    # Check if it needs conversion (might already be 0/1 from get_dummies if category dtype)
    if df_encoded[boolean_predictor].dtype == 'bool':
        df_encoded[boolean_predictor] = df_encoded[boolean_predictor].astype(int)
        print(f"Column '{boolean_predictor}' converted to integer.")

# --- 1d. Verify All Features are Numeric ---
print("\nChecking final predictor dtypes before scaling:")
non_numeric_cols = df_encoded.select_dtypes(exclude=np.number).columns.tolist()
if non_numeric_cols:
      print(f"Warning: Non-numeric columns still present after encoding: {non_numeric_cols}")
      print(df_encoded[non_numeric_cols].info())
else:
      print("All predictor columns are numeric.")


# --- 2. Scale the Data ---
print("\nScaling data using StandardScaler...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_encoded)
print("Data scaling complete. Scaled data shape:", X_scaled.shape)


# --- 3. K-means Clustering ---
# Based on the elbow plot you provided, k=4 seems optimal
k = 4
print(f"\nRunning K-means clustering with k={k}...")

kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto') # Use 'auto' for n_init
cluster_labels = kmeans.fit_predict(X_scaled)
print("K-means fitting complete.")


# --- 4. Add Cluster Labels to Original DataFrame ---
# IMPORTANT: Add labels back to the *original* df for interpretable profiling
df['KM_segments'] = cluster_labels
print(f"\n'KM_segments' column added to original DataFrame 'df'. Value counts:")
print(df['KM_segments'].value_counts().sort_index())


# --- 5. Cluster Profiling (Using Original Data) ---
print("\nCalculating cluster profiles using original data units...")

# Define the original numerical columns you want to average for the profile
# Include JobSatisfaction_Ordinal to see the average satisfaction per cluster
numerical_columns_for_profile = [
    'Age', 'Experience', 'PhysicalActivityHours', 'SleepHours',
    'CommuteDistance', 'NumCompanies', 'TeamSize', 'NumReports',
    'TrainingHoursPerYear',
    'JobSatisfaction_Ordinal' # Add satisfaction score here
    # Add any other ORIGINAL numeric columns of interest
]
# Ensure columns exist in the original df
numerical_columns_for_profile = [col for col in numerical_columns_for_profile if col in df.columns]

if not numerical_columns_for_profile:
    print("Error: No numerical columns found or specified for profiling in the original df.")
else:
    # Group by segments, select ONLY the desired numerical columns, then calculate mean
    km_cluster_profile = df.groupby("KM_segments")[numerical_columns_for_profile].mean()

    # Add counts to the profile table
    # Use a reliable column that exists for all rows (like EmpID if available, or index)
    if "EmpID" in df.columns:
        km_cluster_profile["count_in_each_segment"] = df.groupby("KM_segments")["EmpID"].count()
    else:
        # Fallback to using the index size if EmpID is not present
        km_cluster_profile["count_in_each_segment"] = df.groupby("KM_segments").size()


    print("\n--- Cluster Profiles (Mean Values + Counts) ---")
    # Display the profile (consider formatting for readability if needed)
    # pd.options.display.float_format = '{:.2f}'.format # Optional: format floats
    print(km_cluster_profile)

    print("\n--- Interpretation Notes ---")
    print(f"* Compare the mean 'JobSatisfaction_Ordinal' across the {k} segments.")
    print("* Look at the means of other variables (Age, Experience, etc.) to understand")
    print("    the characteristics of segments with high vs. low satisfaction.")
    print("* Remember higher 'JobSatisfaction_Ordinal' means higher satisfaction.")



km_cluster_profile.style.highlight_min(color = "orange", axis = 0)

import nbformat
from google.colab import _message

# Get current notebook path from Colab's internal message API
def get_notebook_path():
    response = _message.blocking_request('get_ipynb', request='')
    return '/content/' + response['Employee_CSAT.ipynb']

notebook_path = get_notebook_path()
print("Detected notebook:", notebook_path)

# Load and clean
with open(notebook_path, 'r') as f:
    nb = nbformat.read(f, as_version=4)

if 'widgets' in nb.metadata:
    del nb.metadata['widgets']
    print("Removed metadata.widgets")

# Optional: clear cell outputs
for cell in nb.cells:
    if 'outputs' in cell:
        cell['outputs'] = []
    if 'execution_count' in cell:
        cell['execution_count'] = None

# Save cleaned notebook
clean_path = notebook_path.replace('.ipynb', '_cleaned.ipynb')
with open(clean_path, 'w') as f:
    nbformat.write(nb, f)

print(f"Saved cleaned notebook as: {clean_path}")

import nbformat

# Step 1: Set your uploaded notebook file here
uploaded_path = "/content/Employee_CSAT.ipynb"  # ← Replace with your actual filename

# Step 2: Load notebook
with open(uploaded_path) as f:
    nb = nbformat.read(f, as_version=4)

# Step 3: Clean metadata
if "widgets" in nb.get("metadata", {}):
    del nb["metadata"]["widgets"]
    print("Removed 'metadata.widgets'")

# Optional: clear outputs
for cell in nb.cells:
    cell['outputs'] = []
    cell['execution_count'] = None

# Step 4: Save cleaned notebook
cleaned_path = uploaded_path.replace(".ipynb", "_cleaned.ipynb")
with open(cleaned_path, "w") as f:
    nbformat.write(nb, f)

print(f"Cleaned notebook saved to: {cleaned_path}")

